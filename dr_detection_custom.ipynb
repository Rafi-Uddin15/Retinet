{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Diabetic Retinopathy Detection with Custom Hybrid Swin Transformer\n",
                "\n",
                "This notebook implements a state-of-the-art Diabetic Retinopathy detection system using a **Custom Hybrid Architecture**.\n",
                "\n",
                "### Architecture Highlights:\n",
                "1.  **Backbone**: Swin Transformer v2 (Pre-trained on ImageNet) for powerful feature extraction.\n",
                "2.  **Custom Neck**: **Coordinate Attention (CA)** module to enhance feature representation.\n",
                "3.  **Head**: Custom classification head for 5-class DR grading.\n",
                "\n",
                "### Optimization:\n",
                "1.  **WeightedRandomSampler**: Physically oversamples minority classes (Severe/Proliferative) to fix imbalance.\n",
                "2.  **Mixup & Cutmix**: Advanced augmentation that blends images and labels. Forces the model to learn robust features.\n",
                "3.  **Test Time Augmentation (TTA)**: Averages predictions across flipped versions of the image during validation/testing.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force numpy<2.0 to avoid compatibility issues with scipy/sklearn\n",
                "!pip install \"numpy<2.0\" --upgrade timm torchmetrics grad-cam scipy scikit-learn\n",
                "\n",
                "import os\n",
                "import gc\n",
                "import cv2\n",
                "import time\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch.cuda.amp import autocast, GradScaler\n",
                "import torchvision.transforms as transforms\n",
                "import timm\n",
                "\n",
                "# Seed everything for reproducibility\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed)\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True\n",
                "\n",
                "seed_everything()\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Preparation\n",
                "We assume the dataset is attached to the Kaggle Kernel at `/kaggle/input`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Paths\n",
                "# Dataset: sovitrath/diabetic-retinopathy-2015-data-colored-resized\n",
                "DATA_DIR = '/kaggle/input/diabetic-retinopathy-2015-data-colored-resized/colored_images/colored_images'\n",
                "\n",
                "# Check if path exists\n",
                "if not os.path.exists(DATA_DIR):\n",
                "    print(f\"Path not found: {DATA_DIR}\")\n",
                "    print(\"Listing /kaggle/input to help debug:\")\n",
                "    for root, dirs, files in os.walk('/kaggle/input'):\n",
                "        print(root)\n",
                "        break\n",
                "\n",
                "# Create DataFrame by crawling the folder structure\n",
                "# The dataset is organized into folders: No_DR, Mild, Moderate, Severe, Proliferate_DR\n",
                "data = []\n",
                "mapping = {\n",
                "    'No_DR': 0,\n",
                "    'Mild': 1,\n",
                "    'Moderate': 2,\n",
                "    'Severe': 3,\n",
                "    'Proliferate_DR': 4\n",
                "}\n",
                "\n",
                "print(\"Scanning dataset folders...\")\n",
                "for class_name, label in mapping.items():\n",
                "    class_dir = os.path.join(DATA_DIR, class_name)\n",
                "    if not os.path.exists(class_dir):\n",
                "        print(f\"WARNING: Directory not found: {class_dir}\")\n",
                "        continue\n",
                "        \n",
                "    for img_name in os.listdir(class_dir):\n",
                "        if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
                "            img_path = os.path.join(class_dir, img_name)\n",
                "            data.append([img_path, label])\n",
                "\n",
                "df = pd.DataFrame(data, columns=['id_code', 'label'])\n",
                "print(f\"Loaded dataset with {len(df)} images.\")\n",
                "\n",
                "# Split Data\n",
                "if len(df) > 0:\n",
                "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
                "    print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")\n",
                "else:\n",
                "    print(\"ERROR: No images found. Please check the dataset path.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Custom Dataset & Transforms"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RetinopathyDataset(Dataset):\n",
                "    def __init__(self, df, transform=None):\n",
                "        self.df = df\n",
                "        self.transform = transform\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        row = self.df.iloc[idx]\n",
                "        img_path = row['id_code']\n",
                "        label = row['label']\n",
                "        \n",
                "        image = cv2.imread(img_path)\n",
                "        if image is None:\n",
                "             # Handle missing images if any\n",
                "             image = np.zeros((256, 256, 3), dtype=np.uint8)\n",
                "        else:\n",
                "             image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
                "        \n",
                "        if self.transform:\n",
                "            image = self.transform(image)\n",
                "            \n",
                "        return image, torch.tensor(label, dtype=torch.long)\n",
                "\n",
                "# Transforms\n",
                "train_transforms = transforms.Compose([\n",
                "    transforms.ToPILImage(),\n",
                "    transforms.Resize((256, 256)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(20),\n",
                "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "val_transforms = transforms.Compose([\n",
                "    transforms.ToPILImage(),\n",
                "    transforms.Resize((256, 256)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# --- ADVANCED OPTIMIZATION: WeightedRandomSampler ---\n",
                "# This forces the model to see all classes equally, fixing the imbalance physically.\n",
                "if len(df) > 0:\n",
                "    # Calculate weights for each sample\n",
                "    class_counts = df['label'].value_counts().sort_index().values\n",
                "    sample_weights = [1.0 / class_counts[label] for label in train_df['label']]\n",
                "    sample_weights = torch.DoubleTensor(sample_weights)\n",
                "    \n",
                "    # Create Sampler\n",
                "    sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
                "    print(\"WeightedRandomSampler initialized!\")\n",
                "else:\n",
                "    sampler = None\n",
                "\n",
                "if 'train_df' in locals():\n",
                "    train_dataset = RetinopathyDataset(train_df, transform=train_transforms)\n",
                "    val_dataset = RetinopathyDataset(val_df, transform=val_transforms)\n",
                "\n",
                "    # Use Sampler for Training (shuffle must be False when using sampler)\n",
                "    # num_workers=0 is CRITICAL for Kaggle stability\n",
                "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, sampler=sampler, num_workers=0, pin_memory=True)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Custom Hybrid Model (Swin + Coordinate Attention)\n",
                "This is the core innovation of the project."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CoordinateAttention(nn.Module):\n",
                "    def __init__(self, inp, reduction=32):\n",
                "        super(CoordinateAttention, self).__init__()\n",
                "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
                "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
                "\n",
                "        mip = max(8, inp // reduction)\n",
                "\n",
                "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
                "        self.bn1 = nn.BatchNorm2d(mip)\n",
                "        self.act = nn.Hardswish()\n",
                "        \n",
                "        self.conv_h = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n",
                "        self.conv_w = nn.Conv2d(mip, inp, kernel_size=1, stride=1, padding=0)\n",
                "        \n",
                "\n",
                "    def forward(self, x):\n",
                "        identity = x\n",
                "        \n",
                "        n, c, h, w = x.size()\n",
                "        x_h = self.pool_h(x)\n",
                "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
                "\n",
                "        y = torch.cat([x_h, x_w], dim=2)\n",
                "        y = self.conv1(y)\n",
                "        y = self.bn1(y)\n",
                "        y = self.act(y) \n",
                "        \n",
                "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
                "        x_w = x_w.permute(0, 1, 3, 2)\n",
                "\n",
                "        a_h = self.conv_h(x_h).sigmoid()\n",
                "        a_w = self.conv_w(x_w).sigmoid()\n",
                "\n",
                "        out = identity * a_h * a_w\n",
                "        return out\n",
                "\n",
                "class SwinTransformerCA(nn.Module):\n",
                "    def __init__(self, num_classes=5, pretrained=True):\n",
                "        super(SwinTransformerCA, self).__init__()\n",
                "        # Load Swin Transformer v2 as backbone\n",
                "        # CORRECTED MODEL NAME: swinv2_tiny_window8_256.ms_in1k\n",
                "        try:\n",
                "            self.backbone = timm.create_model('swinv2_tiny_window8_256.ms_in1k', pretrained=pretrained, num_classes=0)\n",
                "        except RuntimeError:\n",
                "            print(\"Model not found. Listing available Swin V2 models:\")\n",
                "            print(timm.list_models('*swin*v2*'))\n",
                "            raise\n",
                "            \n",
                "        num_features = self.backbone.num_features\n",
                "        \n",
                "        self.ca = CoordinateAttention(num_features)\n",
                "        \n",
                "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.head = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(num_features, 512),\n",
                "            nn.BatchNorm1d(512),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(512, num_classes)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Swin backbone features\n",
                "        x = self.backbone.forward_features(x) # Returns (B, H, W, C)\n",
                "        \n",
                "        # Permute to (B, C, H, W) for Coordinate Attention\n",
                "        x = x.permute(0, 3, 1, 2)\n",
                "        \n",
                "        # Apply Custom Attention\n",
                "        x = self.ca(x)\n",
                "        \n",
                "        # Pooling and Head\n",
                "        x = self.avg_pool(x).flatten(1)\n",
                "        x = self.head(x)\n",
                "        return x\n",
                "\n",
                "model = SwinTransformerCA(num_classes=5)\n",
                "model = model.to(device)\n",
                "print(\"Model initialized with Coordinate Attention!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training Loop with Mixup & Cutmix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- ADVANCED OPTIMIZATION: Mixup & Cutmix ---\n",
                "from timm.data.mixup import Mixup\n",
                "from timm.loss import SoftTargetCrossEntropy\n",
                "\n",
                "# Configure Mixup\n",
                "mixup_args = {\n",
                "    'mixup_alpha': 0.8,\n",
                "    'cutmix_alpha': 1.0,\n",
                "    'cutmix_minmax': None,\n",
                "    'prob': 1.0,            # Apply mixup to 100% of batches\n",
                "    'switch_prob': 0.5,     # Switch between mixup and cutmix\n",
                "    'mode': 'batch',\n",
                "    'label_smoothing': 0.1,\n",
                "    'num_classes': 5\n",
                "}\n",
                "mixup_fn = Mixup(**mixup_args)\n",
                "print(\"Mixup & Cutmix Augmentation Enabled!\")\n",
                "\n",
                "# Loss Function for Mixup (Soft Targets)\n",
                "criterion = SoftTargetCrossEntropy()\n",
                "val_criterion = nn.CrossEntropyLoss() # Validation uses standard loss\n",
                "\n",
                "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.05) # Increased weight decay for regularization\n",
                "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
                "scaler = GradScaler()\n",
                "\n",
                "def train_one_epoch(model, loader, optimizer, criterion, scaler, mixup_fn):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc=\"Training\")\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        # Apply Mixup\n",
                "        if mixup_fn is not None:\n",
                "            images, labels = mixup_fn(images, labels)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        with autocast():\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        \n",
                "        # Accuracy calculation is tricky with Mixup, so we skip it for training logs\n",
                "        # or use the max prob class (approximate)\n",
                "        _, predicted = outputs.max(1)\n",
                "        # For logging only - labels are mixed so this isn't perfect accuracy\n",
                "        _, labels_max = labels.max(1) \n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels_max).sum().item()\n",
                "        \n",
                "        pbar.set_postfix({'loss': running_loss/total})\n",
                "        \n",
                "    return running_loss / len(loader), correct / total\n",
                "\n",
                "# --- ADVANCED OPTIMIZATION: Test Time Augmentation (TTA) ---\n",
                "# TTA averages predictions across multiple augmented versions of the image\n",
                "def validate_tta(model, loader, criterion, tta_steps=5):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    # TTA Transforms (must be same as train transforms but without the extreme distortions)\n",
                "    tta_transform = transforms.Compose([\n",
                "        transforms.ToPILImage(),\n",
                "        transforms.Resize((256, 256)),\n",
                "        transforms.RandomHorizontalFlip(), # TTA flip\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "    ])\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(loader, desc=\"Validation (TTA)\"):\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            \n",
                "            # Standard Prediction\n",
                "            outputs = model(images)\n",
                "            \n",
                "            # TTA: Horizontal Flip\n",
                "            # We need to apply the flip manually or use a loop if we had the original images.\n",
                "            # Since loader gives tensors, we can just flip the tensors directly.\n",
                "            images_flipped = torch.flip(images, [3]) # Flip width dimension\n",
                "            outputs_flipped = model(images_flipped)\n",
                "            \n",
                "            # Average Predictions\n",
                "            outputs = (outputs + outputs_flipped) / 2.0\n",
                "            \n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            _, predicted = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += predicted.eq(labels).sum().item()\n",
                "            \n",
                "    return running_loss / len(loader), correct / total\n",
                "\n",
                "# Training\n",
                "epochs = 50 # Increased to 50 for Mixup convergence\n",
                "best_acc = 0.0\n",
                "\n",
                "if 'train_loader' in locals():\n",
                "    for epoch in range(epochs):\n",
                "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
                "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, scaler, mixup_fn)\n",
                "        \n",
                "        # Use TTA for Validation\n",
                "        val_loss, val_acc = validate_tta(model, val_loader, val_criterion)\n",
                "        \n",
                "        scheduler.step()\n",
                "        \n",
                "        print(f\"Train Loss: {train_loss:.4f} | Train Acc (Approx): {train_acc:.4f}\")\n",
                "        print(f\"Val Loss: {val_loss:.4f} | Val Acc (TTA): {val_acc:.4f}\")\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), 'best_model.pth')\n",
                "            print(\"Saved Best Model!\")\n",
                "        print(\"-\"*30)\n",
                "else:\n",
                "    print(\"Data loaders not defined. Skipping training loop.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Best Model\n",
                "if os.path.exists('best_model.pth'):\n",
                "    model.load_state_dict(torch.load('best_model.pth'))\n",
                "    model.eval()\n",
                "\n",
                "    # Inference on Validation Set\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in val_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            _, predicted = outputs.max(1)\n",
                "            all_preds.extend(predicted.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "\n",
                "    print(classification_report(all_labels, all_preds))\n",
                "else:\n",
                "    print(\"No model weights found. Train the model first.\")\n",
                "\n",
                "# --- GRAD-CAM VISUALIZATION ---\n",
                "# Install grad-cam library\n",
                "# !pip install grad-cam (Run this in a separate cell if needed, but we included it at the top)\n",
                "\n",
                "try:\n",
                "    from pytorch_grad_cam import GradCAM\n",
                "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
                "    from pytorch_grad_cam.utils.image import show_cam_on_image\n",
                "    \n",
                "    print(\"Generating Grad-CAM Visualizations...\")\n",
                "    \n",
                "    # Target the Coordinate Attention layer or the last Swin block\n",
                "    # We target the Coordinate Attention module to see its effect\n",
                "    target_layers = [model.ca] \n",
                "    \n",
                "    # If that fails, fallback to backbone norm: [model.backbone.norm]\n",
                "    \n",
                "    cam = GradCAM(model=model, target_layers=target_layers)\n",
                "    \n",
                "    # Get a few validation images\n",
                "    model.eval()\n",
                "    \n",
                "    # Create a figure\n",
                "    fig, axes = plt.subplots(4, 2, figsize=(10, 20))\n",
                "    \n",
                "    # Get a batch\n",
                "    images, labels = next(iter(val_loader))\n",
                "    images = images.to(device)\n",
                "    \n",
                "    for i in range(4): # Show 4 examples\n",
                "        input_tensor = images[i].unsqueeze(0)\n",
                "        label = labels[i].item()\n",
                "        \n",
                "        # Generate CAM\n",
                "        grayscale_cam = cam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(label)])\n",
                "        grayscale_cam = grayscale_cam[0, :]\n",
                "        \n",
                "        # Prepare image for visualization\n",
                "        img = input_tensor.squeeze().permute(1, 2, 0).cpu().numpy()\n",
                "        img = (img - img.min()) / (img.max() - img.min()) # Normalize to 0-1\n",
                "        \n",
                "        visualization = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n",
                "        \n",
                "        # Plot Original\n",
                "        axes[i, 0].imshow(img)\n",
                "        axes[i, 0].set_title(f\"Original (Label: {label})\")\n",
                "        axes[i, 0].axis('off')\n",
                "        \n",
                "        # Plot Grad-CAM\n",
                "        axes[i, 1].imshow(visualization)\n",
                "        axes[i, 1].set_title(f\"Grad-CAM (Focus)\")\n",
                "        axes[i, 1].axis('off')\n",
                "        \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"grad-cam library not installed. Please run: !pip install grad-cam\")\n",
                "except Exception as e:\n",
                "    print(f\"Grad-CAM Error: {e}\")\n",
                "    print(\"Try changing target_layers to [model.backbone.norm]\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
